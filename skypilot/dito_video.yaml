name: owl-vaes-multinode

resources:
  cloud: kubernetes
  accelerators: H200:8
  network_tier: best
  
num_nodes: 4

setup: |
  sudo apt-get update
  sudo apt-get install -y libgl1
  
run: |
  cd /mnt/data/shahbuland
  source venv/bin/activate
  cd owl-vaes

  # Get the head node IP
  MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  echo "Starting distributed training, head node: $MASTER_ADDR"

  # NCCL optimizations for socket-based communication
  export NCCL_SOCKET_IFNAME=eth0  # Use ethernet interface
  export NCCL_IB_DISABLE=1         # Disable InfiniBand to force sockets
  export NCCL_P2P_DISABLE=0        # Enable P2P for intra-node
  export NCCL_SHM_DISABLE=0        # Enable shared memory for intra-node
  export NCCL_SOCKET_NTHREADS=4    # More threads for socket operations
  export NCCL_NSOCKS_PERTHREAD=4   # More sockets per thread
  export NCCL_BUFFSIZE=2097152     # 2MB buffer size for better throughput

  # Run distributed training with torchrun
  torchrun \
    --nnodes=$SKYPILOT_NUM_NODES \
    --nproc_per_node=$SKYPILOT_NUM_GPUS_PER_NODE \
    --master_addr=$MASTER_ADDR \
    --node_rank=${SKYPILOT_NODE_RANK} \
    --master_port=8008 \
    train.py --config_path configs/dito/wp1_video_dito.yml